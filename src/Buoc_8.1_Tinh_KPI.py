# -*- coding: utf-8 -*-
"""
B∆∞·ªõc 8.1 - Ph√¢n t√≠ch k·∫øt qu·∫£ gh√©p c·∫∑p t·ª´ b∆∞·ªõc 6
üéØ M·ª•c ti√™u: Hi·ªÉn th·ªã chi ti·∫øt v√† ƒë√°nh gi√° c√°c c·∫∑p gh√©p t·ª± ƒë·ªông
üìù Input:
    - output/matched_pairs.csv: File k·∫øt qu·∫£ t·ª´ b∆∞·ªõc 6 (c√°c c·∫∑p ƒë√£ gh√©p)
    - data/answer_key_sample.csv: File ƒë√°p √°n ƒë·ªÉ so s√°nh
ÔøΩ Output:
    - Danh s√°ch chi ti·∫øt c√°c c·∫∑p matched
    - Th·ªëng k√™ t·ªïng h·ª£p (s·ªë l∆∞·ª£ng, t·ª∑ l·ªá)
    - Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng gh√©p (KPIs)
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime
from pathlib import Path

# === THI·∫æT L·∫¨P ƒê∆Ø·ªúNG D·∫™N ===
script_dir = Path(__file__).resolve().parent
root_dir = script_dir.parent
data_dir = root_dir / "data"
output_dir = root_dir / "output"

# ƒê·∫£m b·∫£o c√°c th∆∞ m·ª•c t·ªìn t·∫°i
output_dir.mkdir(parents=True, exist_ok=True)
data_dir.mkdir(parents=True, exist_ok=True)

# T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
output_dir.mkdir(exist_ok=True)

# === THI·∫æT L·∫¨P T√äN C√ÅC T·ªÜP ===
MATCHED_FILE = output_dir / "matched_pairs.csv"
ANSWER_KEY = data_dir / "answer_key_sample.csv"
KPI_REPORT = output_dir / "kpi_report_8.1.csv"
KPI_DETAILS = output_dir / "kpi_details_8.1.csv"

def calculate_metrics(all_transactions, truth_pairs):
    """T√≠nh to√°n c√°c ch·ªâ s·ªë Precision, Recall"""
    
    # L·ªçc c√°c c·∫∑p ƒë√£ matched
    system_pairs = all_transactions[all_transactions['status'] == 'matched']
    
    # T√≠nh True Positives (gh√©p ƒë√∫ng)
    merged_df = pd.merge(system_pairs, truth_pairs, 
                        on=['bank_ref', 'gl_doc'], 
                        how='inner')
    true_positives = len(merged_df)
    
    # T√≠nh False Positives (gh√©p sai)
    false_positives = len(system_pairs) - true_positives
    
    # T√≠nh False Negatives (b·ªè s√≥t)
    false_negatives = len(truth_pairs) - true_positives
    
    # T√≠nh Precision
    precision = (true_positives / len(system_pairs) * 100) if len(system_pairs) > 0 else 0
    
    # T√≠nh Recall
    recall = (true_positives / len(truth_pairs) * 100) if len(truth_pairs) > 0 else 0
    
    # T√≠nh F1-Score
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    return {
        'true_positives': true_positives,
        'false_positives': false_positives,
        'false_negatives': false_negatives,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score
    }

def analyze_results(all_transactions, truth_df, total_transactions):
    """Ph√¢n t√≠ch chi ti·∫øt k·∫øt qu·∫£ ƒë·ªëi so√°t"""
    
    # 1. T√≠nh Auto-match rate
    matched_pairs = all_transactions[all_transactions['status'] == 'matched']
    auto_matched = len(matched_pairs)
    auto_match_rate = (auto_matched / total_transactions) * 100 if total_transactions > 0 else 0
    
    # 2. T√≠nh c√°c metric cho t·ª´ng nh√≥m
    matched_metrics = calculate_metrics(all_transactions, truth_df)
    
    # 3. Ph√¢n t√≠ch chi ti·∫øt
    details = []
    
    # Chi ti·∫øt c√°c c·∫∑p gh√©p ƒë√∫ng
    true_matches = pd.merge(
        all_transactions[all_transactions['status'] == 'matched'],
        truth_df,
        on=['bank_ref', 'gl_doc'],
        how='inner'
    )
    for _, row in true_matches.iterrows():
        details.append({
            'bank_ref': row['bank_ref'],
            'gl_doc': row['gl_doc'],
            'status': 'TRUE_POSITIVE',
            'reason': 'Gh√©p ƒë√∫ng theo answer key'
        })
    
    # Chi ti·∫øt c√°c c·∫∑p gh√©p sai
    matched_only = all_transactions[all_transactions['status'] == 'matched']
    false_matches = matched_only.merge(
        truth_df,
        on=['bank_ref', 'gl_doc'],
        how='left',
        indicator=True
    )
    false_matches = false_matches[false_matches['_merge'] == 'left_only']
    for _, row in false_matches.iterrows():
        details.append({
            'bank_ref': row['bank_ref'],
            'gl_doc': row['gl_doc'],
            'status': 'FALSE_POSITIVE',
            'reason': 'Gh√©p kh√¥ng kh·ªõp v·ªõi answer key'
        })
    
    # Chi ti·∫øt c√°c c·∫∑p b·ªè s√≥t
    missed_matches = truth_df.merge(
        matched_only,
        on=['bank_ref', 'gl_doc'],
        how='left',
        indicator=True
    )
    missed_matches = missed_matches[missed_matches['_merge'] == 'left_only']
    for _, row in missed_matches.iterrows():
        details.append({
            'bank_ref': row['bank_ref'],
            'gl_doc': row['gl_doc'],
            'status': 'FALSE_NEGATIVE',
            'reason': 'C·∫∑p gh√©p ƒë√∫ng b·ªã b·ªè s√≥t'
        })
    
    return {
        'auto_match_rate': auto_match_rate,
        'metrics': matched_metrics,
        'details': pd.DataFrame(details)
    }

def main():
    """H√†m ch√≠nh ƒë·ªÉ hi·ªÉn th·ªã v√† ph√¢n t√≠ch k·∫øt qu·∫£ gh√©p c·∫∑p"""
    print("\n=== B∆Ø·ªöC 8.1: PH√ÇN T√çCH K·∫æT QU·∫¢ GH√âP C·∫∂P ===")
    
    # 1. Ki·ªÉm tra file b·∫Øt bu·ªôc
    required_files = [MATCHED_FILE, ANSWER_KEY]
    for f in required_files:
        if not f.exists():
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file b·∫Øt bu·ªôc: {f}")
            return
    
    try:
        # 2. ƒê·ªçc d·ªØ li·ªáu
        print("\nüìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu...")
        
        # ƒê·ªçc file matched pairs v√† answer key
        matched_df = pd.read_csv(MATCHED_FILE)
        truth_df = pd.read_csv(ANSWER_KEY)

        # 3. Hi·ªÉn th·ªã danh s√°ch c√°c c·∫∑p matched
        print("\nüìã DANH S√ÅCH C√ÅC C·∫∂P GH√âP TH√ÄNH C√îNG:")
        print("=" * 100)
        print("   Bank Ref         GL Doc      Score     Status")
        print("-" * 100)
        
        matched_only = matched_df[matched_df['match_type'] == 'MATCH'].sort_values(by='match_score', ascending=False)
        for _, row in matched_only.iterrows():
            print(f"   {row['bank_ref']:<15} {row['gl_doc']:<10} {row['match_score']:>8.3f}   {row['match_type']}")
        
        # 4. Hi·ªÉn th·ªã th·ªëng k√™ t·ªïng h·ª£p
        print("\nüìä TH·ªêNG K√ä T·ªîNG H·ª¢P:")
        print("=" * 50)
        
        # ƒê·ªçc c√°c file ngu·ªìn
        bank_df = pd.read_csv(data_dir / "bank_stmt.csv")
        gl_df = pd.read_csv(data_dir / "gl_entries.csv")
        truth_df = pd.read_csv(data_dir / "answer_key_sample.csv")
        
        # T√≠nh to√°n d·ª±a tr√™n answer key
        true_matches_count = len(truth_df)  # S·ªë c·∫∑p match th·ª±c t·∫ø trong answer key
        bank_count = len(bank_df)
        gl_count = len(gl_df)
        
        total_pairs = len(matched_df)
        matched_count = len(matched_only)
        
        print(f"T·ªïng s·ªë giao d·ªãch      : {bank_count:>5} (Bank: {bank_count}, GL: {gl_count})")
        print(f"S·ªë c·∫∑p th·ª±c t·∫ø (answer): {true_matches_count:>5}")
        print(f"S·ªë c·∫∑p gh√©p th√†nh c√¥ng  : {matched_count:>5}")
        print(f"Auto-match rate         : {matched_count/true_matches_count*100:>5.1f}%")
        print(f"T·ª∑ l·ªá gh√©p th√†nh c√¥ng   : {matched_count/total_pairs*100:>5.1f}%")
        
        # 5. Hi·ªÉn th·ªã ph√¢n b·ªë ƒëi·ªÉm s·ªë
        print("\nüìà PH√ÇN B·ªê ƒêI·ªÇM S·ªê:")
        print("=" * 50)
        
        score_bins = [0.75, 0.8, 0.85, 0.9, 0.95, 1.0]
        score_counts = pd.cut(matched_only['match_score'], bins=score_bins).value_counts().sort_index()
        
        for interval, count in score_counts.items():
            print(f"ƒêi·ªÉm {interval.left:.2f} - {interval.right:.2f}: {count:>3} c·∫∑p")
        
        # 6. ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng gh√©p
        print("\nüéØ ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG GH√âP:")
        print("=" * 50)
        
        # So s√°nh v·ªõi answer key
        true_matches = pd.merge(matched_only, truth_df, on=['bank_ref', 'gl_doc'], how='inner')
        precision = len(true_matches) / len(matched_only) * 100
        recall = len(true_matches) / len(truth_df) * 100
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        print(f"Precision (ƒë·ªô ch√≠nh x√°c) : {precision:>5.1f}%")
        print(f"Recall (ƒë·ªô ph·ªß)          : {recall:>5.1f}%")
        print(f"F1-Score (ƒëi·ªÉm t·ªïng h·ª£p) : {f1_score:>5.1f}%")
        
        print("\n‚ú® Ho√†n t·∫•t ph√¢n t√≠ch k·∫øt qu·∫£ gh√©p c·∫∑p!")
        
        # 7. L∆∞u b√°o c√°o chi ti·∫øt n·∫øu c·∫ßn
        detailed_report = {
            'Metric': ['Precision', 'Recall', 'F1-Score'],
            'Value': [precision, recall, f1_score],
            'Description': [
                f'T·ªâ l·ªá gh√©p ƒë√∫ng ({len(true_matches)} / {len(matched_only)} c·∫∑p)',
                f'T·ªâ l·ªá t√¨m th·∫•y ({len(true_matches)} / {len(truth_df)} c·∫∑p)',
                'ƒêi·ªÉm trung b√¨nh ƒëi·ªÅu h√≤a'
            ]
        }
        
        # 8. L∆∞u b√°o c√°o chi ti·∫øt
        pd.DataFrame(detailed_report).to_csv(KPI_REPORT, index=False)
        
        print(f"\n‚úì ƒê√£ l∆∞u b√°o c√°o chi ti·∫øt v√†o: {KPI_REPORT}")

        # 9. Xu·∫•t 3 file ri√™ng bi·ªát cho MATCH, REVIEW, UNMATCHED
        print("\nüì§ ƒêang xu·∫•t c√°c file ph√¢n lo·∫°i...")
        
        # File MATCH
        match_pairs = matched_df[matched_df['match_type'] == 'MATCH']
        match_file = output_dir / 'matched_pairs_MATCH.csv'
        match_pairs.to_csv(match_file, index=False)
        print(f"‚úÖ ƒê√£ xu·∫•t {len(match_pairs)} c·∫∑p MATCH v√†o: {match_file}")
        
        # File REVIEW  
        review_pairs = matched_df[matched_df['match_type'] == 'REVIEW']
        review_file = output_dir / 'matched_pairs_REVIEW.csv'
        review_pairs.to_csv(review_file, index=False)
        print(f"‚úÖ ƒê√£ xu·∫•t {len(review_pairs)} c·∫∑p REVIEW v√†o: {review_file}")
        
        # File UNMATCHED
        unmatched_pairs = matched_df[matched_df['match_type'] == 'UNMATCHED']
        unmatched_file = output_dir / 'matched_pairs_UNMATCHED.csv'
        unmatched_pairs.to_csv(unmatched_file, index=False)
        print(f"‚úÖ ƒê√£ xu·∫•t {len(unmatched_pairs)} c·∫∑p UNMATCHED v√†o: {unmatched_file}")

    except Exception as e:
        print(f"‚ùå L·ªói: {str(e)}")
        raise e

if __name__ == "__main__":
    start_time = datetime.now()
    main()
    end_time = datetime.now()
    
    # T√≠nh v√† hi·ªÉn th·ªã latency
    processing_time = (end_time - start_time).total_seconds()
    print(f"\nLatency: {processing_time:.2f} seconds")